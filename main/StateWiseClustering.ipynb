{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import datetime as dt\n",
    "%matplotlib inline\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 15, 6\n",
    "import random\n",
    "import pprint\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading data from preprocessed file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   State Code  County Code  Site Num                                  Address  \\\n",
      "0           4           13      3002  1645 E ROOSEVELT ST-CENTRAL PHOENIX STN   \n",
      "1           4           13      3002  1645 E ROOSEVELT ST-CENTRAL PHOENIX STN   \n",
      "2           4           13      3002  1645 E ROOSEVELT ST-CENTRAL PHOENIX STN   \n",
      "3           4           13      3002  1645 E ROOSEVELT ST-CENTRAL PHOENIX STN   \n",
      "4           4           13      3002  1645 E ROOSEVELT ST-CENTRAL PHOENIX STN   \n",
      "\n",
      "     State    County     City  Date Local          NO2 Units   NO2 Mean  ...  \\\n",
      "0  Arizona  Maricopa  Phoenix  2000-01-01  Parts per billion  19.041667  ...   \n",
      "1  Arizona  Maricopa  Phoenix  2000-01-01  Parts per billion  19.041667  ...   \n",
      "2  Arizona  Maricopa  Phoenix  2000-01-01  Parts per billion  19.041667  ...   \n",
      "3  Arizona  Maricopa  Phoenix  2000-01-01  Parts per billion  19.041667  ...   \n",
      "4  Arizona  Maricopa  Phoenix  2000-01-02  Parts per billion  22.958333  ...   \n",
      "\n",
      "           SO2 Units  SO2 Mean  SO2 1st Max Value SO2 1st Max Hour    SO2 AQI  \\\n",
      "0  Parts per billion  3.000000                9.0               21  13.000000   \n",
      "1  Parts per billion  3.000000                9.0               21  13.000000   \n",
      "2  Parts per billion  2.975000                6.6               23   5.333333   \n",
      "3  Parts per billion  2.975000                6.6               23   5.333333   \n",
      "4  Parts per billion  1.958333                3.0               22   4.000000   \n",
      "\n",
      "            CO Units   CO Mean  CO 1st Max Value CO 1st Max Hour     CO AQI  \n",
      "0  Parts per million  1.145833               4.2              21  13.666667  \n",
      "1  Parts per million  0.878947               2.2              23  25.000000  \n",
      "2  Parts per million  1.145833               4.2              21  13.666667  \n",
      "3  Parts per million  0.878947               2.2              23  25.000000  \n",
      "4  Parts per million  0.850000               1.6              23  11.666667  \n",
      "\n",
      "[5 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('../data/prepocessed.csv', encoding='utf8', engine='python')\n",
    "data = data.drop(data.columns[0], axis=1)\n",
    "\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating separate files of timeseries for each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'state Data/Arizona.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/y9/mpyb8cg90y57fcjl50m4kg940000gn/T/ipykernel_25040/3879679671.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdatatemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatatemp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Date Local'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdatatemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatatemp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Date Local'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'NO2 Mean'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mdatatemp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3464\u001b[0m         )\n\u001b[1;32m   3465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3466\u001b[0;31m         return DataFrameRenderer(formatter).to_csv(\n\u001b[0m\u001b[1;32m   3467\u001b[0m             \u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3468\u001b[0m             \u001b[0mline_terminator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mline_terminator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/formats/format.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m   1103\u001b[0m             \u001b[0mformatter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfmt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         )\n\u001b[0;32m-> 1105\u001b[0;31m         \u001b[0mcsv_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1107\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcreated_buffer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    235\u001b[0m         \"\"\"\n\u001b[1;32m    236\u001b[0m         \u001b[0;31m# apply compression and byte/text conversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m         with get_handle(\n\u001b[0m\u001b[1;32m    238\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'state Data/Arizona.csv'"
     ]
    }
   ],
   "source": [
    "for state in states:\n",
    "    filename = \"state Data/\"+state + \".csv\"\n",
    "    datatemp = data.loc[data['State'] == state]\n",
    "    datatemp = datatemp.groupby(['Date Local']).mean().reset_index()\n",
    "    datatemp = datatemp[['Date Local','NO2 Mean']].copy()\n",
    "    datatemp.to_csv(filename, ',', encoding='utf8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### creating list of timeseries', later to be fed in algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeSeriesList = []\n",
    "j=0\n",
    "for state in states:\n",
    "    filename = \"../state Data/\"+state+\".csv\"\n",
    "    datatemp = pd.read_csv(filename, usecols=['NO2 Mean'])\n",
    "    dataList = datatemp['NO2 Mean'].tolist()\n",
    "    seriesNP = np.asarray(dataList)\n",
    "    timeSeriesList.append(dataList[0:200])\n",
    "    j=j+1\n",
    "finaltimeSeriesList = np.asarray(timeSeriesList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dynamic Time Wrapping & LB_Keogh for lower bounding of DTW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DynamicTimeWarping(timeseries, centroid_timeseries, window):\n",
    "    DTW_Dictionary={}\n",
    "    \n",
    "    for i in range(0,(len(timeseries)+1)):\n",
    "        for j in range(0,(len(centroid_timeseries)+1)):\n",
    "            if((i==0) & (j==0)):\n",
    "                DTW_Dictionary[(i, j)] = float(0)\n",
    "            else:\n",
    "                DTW_Dictionary[(i, j)] = float('inf')    \n",
    "       \n",
    "    for item in range(len(timeseries)):\n",
    "        low = (item-window) if (item-window)>=0 else 0\n",
    "        high = (len(centroid_timeseries)) if ((item+window) >= (len(centroid_timeseries))) else (item+window)\n",
    "        for second_item in range(low, high):    \n",
    "            distance= (timeseries[item]-centroid_timeseries[second_item])**2\n",
    "            DTW_Dictionary[(item+1), (second_item+1)] = distance + min(DTW_Dictionary[(item, second_item+1)],DTW_Dictionary[(item+1, second_item)], DTW_Dictionary[(item, second_item)])\n",
    "    return np.sqrt(DTW_Dictionary[len(timeseries), len(centroid_timeseries)])\n",
    "\n",
    "def LowerBoundKeogh(timeseries,centroid_timeseries,reach):\n",
    "    lowerBoundSum=0\n",
    "    for index, item in enumerate(timeseries):\n",
    "        low = 0 if (index-reach)<0 else (index-reach)\n",
    "        if len(centroid_timeseries[(low):(index+reach)])==0:\n",
    "            break\n",
    "        lowerBound = min(centroid_timeseries[(low):(index+reach)])\n",
    "        upperBound = max(centroid_timeseries[(low):(index+reach)])\n",
    "        \n",
    "        if item>upperBound:\n",
    "            lowerBoundSum = lowerBoundSum+(item-upperBound)**2\n",
    "        elif item<lowerBound:\n",
    "            lowerBoundSum = lowerBoundSum+(item-lowerBound)**2\n",
    "    \n",
    "    return np.sqrt(lowerBoundSum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Means Clustering. Args - Data, number of clusters, number of iterations, window for DynamicTimeWarping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KMeans(data, numCluster, iterations, window):\n",
    "    centroids = random.sample(list(data),numCluster)\n",
    "\n",
    "    for n in range(iterations):\n",
    "        print(n)\n",
    "        clusterID = {}\n",
    "        \n",
    "        for index, item in enumerate(data):\n",
    "            minDist = float('inf')\n",
    "            closestCluster = None\n",
    "            for centroidIndex, j in enumerate(centroids):\n",
    "                if LowerBoundKeogh(item, j, 5) < minDist:\n",
    "                    currentDist = DynamicTimeWarping(item, j, window)\n",
    "                    if currentDist < minDist:\n",
    "                        minDist = currentDist\n",
    "                        closestCluster = centroidIndex\n",
    "                       \n",
    "            if closestCluster in clusterID:\n",
    "                clusterID[closestCluster].append(index)\n",
    "            else:\n",
    "                clusterID[closestCluster]=[]\n",
    "\n",
    "        for ID in clusterID:\n",
    "            clusterSum = [0]*200\n",
    "            if ID is not None:\n",
    "                for k in clusterID[ID]:\n",
    "                    clusterSum = clusterSum + data[k]   \n",
    "                centroids[ID] = [m/len(clusterID[ID]) for m in clusterSum if len(clusterID[ID])!=0]\n",
    "    \n",
    "    return centroids, clusterID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids, clusterID = KMeans(finaltimeSeriesList, 4, 10, 5)\n",
    "\n",
    "for i in centroids:\n",
    "    plt.plot(i)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Results of clustering state-wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = {}\n",
    "for i in clusterID:\n",
    "    list = clusterID[i]\n",
    "    tempList = []\n",
    "    for state in list:\n",
    "        tempList.append(states[state])\n",
    "    dictionary[i] = tempList\n",
    "pprint.pprint(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting cluster centroids into time-series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = dt.datetime(2000, 1, 1, 0, 0, 0)\n",
    "datelist = pd.date_range(base, periods=200).tolist()\n",
    "\n",
    "cluster1 = pd.DataFrame({'Date':datelist, 'NO2 Mean':centroids[0]})\n",
    "cluster1['Date'] = pd.to_datetime(cluster1['Date'], format='%Y-%m-%d')\n",
    "\n",
    "\n",
    "cluster2 = pd.DataFrame({'Date':datelist, 'NO2 Mean':centroids[1]})\n",
    "cluster2['Date'] = pd.to_datetime(cluster2['Date'], format='%Y-%m-%d')\n",
    "\n",
    "cluster3 = pd.DataFrame({'Date':datelist, 'NO2 Mean':centroids[2]})\n",
    "cluster3['Date'] = pd.to_datetime(cluster3['Date'], format='%Y-%m-%d')\n",
    "\n",
    "cluster4 = pd.DataFrame({'Date':datelist, 'NO2 Mean':centroids[3]})\n",
    "cluster4['Date'] = pd.to_datetime(cluster4['Date'], format='%Y-%m-%d')\n",
    "\n",
    "cluster1TS = cluster1.set_index('Date')\n",
    "cluster2TS = cluster2.set_index('Date')\n",
    "cluster3TS = cluster3.set_index('Date')\n",
    "cluster4TS = cluster4.set_index('Date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Generating graph for each clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label=\", \".join(str(x) for x in dictionary[0])\n",
    "plt.plot(cluster1TS, label=label)\n",
    "plt.legend(loc=\"best\")\n",
    "plt.title(\"Cluster 1\")\n",
    "plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label=\", \".join(str(x) for x in dictionary[1])\n",
    "plt.plot(cluster2TS, label=label, color='red')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.title(\"Cluster 2\")\n",
    "plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label=\", \".join(str(x) for x in dictionary[2])\n",
    "plt.plot(cluster3TS, label=label, color='black')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.title(\"Cluster 3\")\n",
    "plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label=\", \".join(str(x) for x in dictionary[3])\n",
    "plt.plot(cluster4TS, label=label, color='green')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.title(\"Cluster 4\")\n",
    "plt.show(block=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ARIMA on cluster 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster1Values = cluster1TS.values\n",
    "train, test = cluster1Values[0:int(len(cluster1Values) * 0.80)], cluster1Values[int(len(cluster1Values) * 0.80):len(cluster1Values)]\n",
    "trainingData = [x for x in train]\n",
    "predictions = []\n",
    "\n",
    "for item in range(len(test)):\n",
    "    model = ARIMA(trainingData, order=(5,1,0))\n",
    "    model_fit = model.fit(disp=0)\n",
    "    output = model_fit.forecast()\n",
    "    predicted = output[0]\n",
    "    predictions.append(predicted)\n",
    "    observation = test[item]\n",
    "    trainingData.append(observation)\n",
    "    print('predicted=%f, expected=%f' % (predicted, observation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = mean_squared_error(test, predictions)\n",
    "print('Test MSE: %.3f' % error)\n",
    "\n",
    "plt.plot(test)\n",
    "plt.plot(predictions, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intraClusterError = {}\n",
    "interDistance = 0\n",
    "\n",
    "for j in clusterID.keys():\n",
    "    distance = 0\n",
    "    for i in clusterID[j]:\n",
    "        temp = finaltimeSeriesList[j][0:200]\n",
    "        tempCentroid = centroids[j]\n",
    "        distance = distance + DynamicTimeWarping(temp, tempCentroid, 5)\n",
    "    intraClusterError[j] = distance\n",
    "    \n",
    "intraDistance = sum(intraClusterError.values())\n",
    "\n",
    "for i in range(4):\n",
    "    for j in range(i+1,4):\n",
    "        interDistance = interDistance + DynamicTimeWarping(centroids[i], centroids[j], 5)\n",
    "        \n",
    "variance = interDistance + intraDistance\n",
    "\n",
    "print('Intra Cluster Distance: %.3f' % intraDistance)\n",
    "print('Inter Cluster Distance: %.3f' % interDistance)\n",
    "print('Total Sum Square (Variance): %.3f' % variance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
